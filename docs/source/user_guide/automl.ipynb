{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning (AutoML) Search\n",
    "\n",
    "## Background\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "[Machine learning](https://en.wikipedia.org/wiki/Machine_learning) (ML) is the process of constructing a mathematical model of a system based on a sample dataset collected from that system.\n",
    "\n",
    "One of the main goals of training an ML model is to teach the model to separate the signal present in the data from the noise inherent in system and in the data collection process. If this is done effectively, the model can then be used to make accurate predictions about the system when presented with new, similar data. Additionally, introspecting on an ML model can reveal key information about the system being modeled, such as which inputs and transformations of the inputs are most useful to the ML model for learning the signal in the data, and are therefore the most predictive.\n",
    "\n",
    "There are [a variety](https://en.wikipedia.org/wiki/Machine_learning#Approaches) of ML problem types. Supervised learning describes the case where the collected data contains an output value to be modeled and a set of inputs with which to train the model. EvalML focuses on training supervised learning models.\n",
    "\n",
    "EvalML supports three common supervised ML problem types. The first is regression, where the target value to model is a continuous numeric value. Next are binary and multiclass classification, where the target value to model consists of two or more discrete values or categories. The choice of which supervised ML problem type is most appropriate depends on domain expertise and on how the model will be evaluated and used. \n",
    "\n",
    "EvalML is currently building support for supervised time series problems: time series regression, time series binary classification, and time series multiclass classification. While we've added some features to tackle these kinds of problems, our functionality is still being actively developed so please be mindful of that before using it. \n",
    "\n",
    "\n",
    "### AutoML and Search\n",
    "\n",
    "[AutoML](https://en.wikipedia.org/wiki/Automated_machine_learning) is the process of automating the construction, training and evaluation of ML models. Given a data and some configuration, AutoML searches for the most effective and accurate ML model or models to fit the dataset. During the search, AutoML will explore different combinations of model type, model parameters and model architecture.\n",
    "\n",
    "An effective AutoML solution offers several advantages over constructing and tuning ML models by hand. AutoML can assist with many of the difficult aspects of ML, such as avoiding overfitting and underfitting, imbalanced data, detecting data leakage and other potential issues with the problem setup, and automatically applying best-practice data cleaning, feature engineering, feature selection and various modeling techniques. AutoML can also leverage search algorithms to optimally sweep the hyperparameter search space, resulting in model performance which would be difficult to achieve by manual training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML in EvalML\n",
    "\n",
    "EvalML supports all of the above and more.\n",
    "\n",
    "In its simplest usage, the AutoML search interface requires only the input data, the target data and a `problem_type` specifying what kind of supervised ML problem to model.\n",
    "\n",
    "** Graphing methods, like AutoMLSearch, on Jupyter Notebook and Jupyter Lab require [ipywidgets](https://ipywidgets.readthedocs.io/en/latest/user_install.html) to be installed.\n",
    "\n",
    "** If graphing on Jupyter Lab, [jupyterlab-plotly](https://plotly.com/python/getting-started/#jupyterlab-support-python-35) required. To download this, make sure you have [npm](https://nodejs.org/en/download/) installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:21.895973Z",
     "start_time": "2021-07-27T18:55:21.317120Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 250\n",
      "Targets\n",
      "False    88.40%\n",
      "True     11.60%\n",
      "Name: fraud, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import evalml\n",
    "from evalml.utils import infer_feature_types\n",
    "X, y = evalml.demos.load_fraud(n_rows=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide data to EvalML, it is recommended that you initialize a [Woodwork accessor](https://woodwork.alteryx.com/en/stable/) on your data. This allows you to easily control how EvalML will treat each of your features before training a model.\n",
    "\n",
    "EvalML also accepts ``pandas`` input, and will run type inference on top of the input ``pandas`` data. If you'd like to change the types inferred by EvalML, you can use the `infer_feature_types` utility method, which takes pandas or numpy input and converts it to a Woodwork data structure. The `feature_types` parameter can be used to specify what types specific columns should be.\n",
    "\n",
    "Feature types such as `Natural Language` must be specified in this way, otherwise Woodwork will infer it as `Unknown` type and drop it during the AutoMLSearch.\n",
    "\n",
    "In the example below, we reformat a couple features to make them easily consumable by the model, and then specify that the provider, which would have otherwise been inferred as a column with natural language, is a categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:21.904882Z",
     "start_time": "2021-07-27T18:55:21.898320Z"
    }
   },
   "outputs": [],
   "source": [
    "X.ww['expiration_date'] = X['expiration_date'].apply(lambda x: '20{}-01-{}'.format(x.split(\"/\")[1], x.split(\"/\")[0]))\n",
    "X = infer_feature_types(X, feature_types= {'store_id': 'categorical',\n",
    "                                           'expiration_date': 'datetime',\n",
    "                                           'lat': 'categorical',\n",
    "                                           'lng': 'categorical',\n",
    "                                           'provider': 'categorical'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate the results of the pipeline creation and optimization process, we will save some of our data as a holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:21.914736Z",
     "start_time": "2021-07-27T18:55:21.906331Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = evalml.preprocessing.split_data(X, y, problem_type='binary', test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Checks\n",
    "\n",
    "Before calling `AutoMLSearch.search`, we should run some sanity checks on our data to ensure that the input data being passed will not run into some common issues before running a potentially time-consuming search. EvalML has various data checks that makes this easy. Each data check will return a collection of warnings and errors if it detects potential issues with the input data. This allows users to inspect their data to avoid confusing errors that may arise during the search process. You can learn about each of the data checks available through our [data checks guide](data_checks.ipynb) \n",
    "\n",
    "Here, we will run the `DefaultDataChecks` class, which contains a series of data checks that are generally useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:22.037826Z",
     "start_time": "2021-07-27T18:55:21.917798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'warnings': [], 'errors': [], 'actions': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evalml.data_checks import DefaultDataChecks\n",
    "\n",
    "data_checks = DefaultDataChecks(\"binary\", \"log loss binary\")\n",
    "data_checks.validate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there were no warnings or errors returned, we can safely continue with the search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.252138Z",
     "start_time": "2021-07-27T18:55:22.039174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "8 pipelines ready for search.\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using SequentialEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: extra_trees, lightgbm, linear_model, xgboost, random_forest, decision_tree, catboost\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b054bdc9780344e09054c08246e3a5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Best Score',\n",
       "              'type'\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 3.970\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.512\n",
      "Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 2.957\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler may not perform as estimated on unseen data.\n",
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.286\n",
      "LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.309\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler may not perform as estimated on unseen data.\n",
      "Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:55:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\t\t\tFold 0: Encountered an error.\n",
      "\t\t\tFold 0: All scores will be replaced with nan.\n",
      "\t\t\tFold 0: Please check the log file for the current hyperparameters and stack trace.\n",
      "\t\t\tFold 0: Exception during automl search: [14:55:38] ../src/c_api/../data/array_interface.h:139: Check failed: typestr.size() == 3 (2 vs. 3) : `typestr' should be of format <endian><type><size of type in bytes>.\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x00000001a2b2b074 dmlc::LogMessageFatal::~LogMessageFatal() + 116\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x00000001a2b25773 xgboost::ArrayInterfaceHandler::Validate(std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, xgboost::Json, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, xgboost::Json> > > const&) + 835\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x00000001a2b24e30 xgboost::ArrayInterface::Initialize(std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, xgboost::Json, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, xgboost::Json> > > const&, bool) + 48\n",
      "  [bt] (3) 4   libxgboost.dylib                    0x00000001a2b32d81 xgboost::data::ArrayAdapter::ArrayAdapter(xgboost::StringView) + 161\n",
      "  [bt] (4) 5   libxgboost.dylib                    0x00000001a2b321c4 XGBoosterPredictFromDense + 164\n",
      "  [bt] (5) 6   libffi.7.dylib                      0x0000000103127e9d ffi_call_unix64 + 85\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:55:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\t\t\tFold 1: Encountered an error.\n",
      "\t\t\tFold 1: All scores will be replaced with nan.\n",
      "\t\t\tFold 1: Please check the log file for the current hyperparameters and stack trace.\n",
      "\t\t\tFold 1: Exception during automl search: [14:55:38] ../src/c_api/../data/array_interface.h:139: Check failed: typestr.size() == 3 (2 vs. 3) : `typestr' should be of format <endian><type><size of type in bytes>.\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x00000001a2b2b074 dmlc::LogMessageFatal::~LogMessageFatal() + 116\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x00000001a2b25773 xgboost::ArrayInterfaceHandler::Validate(std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, xgboost::Json, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, xgboost::Json> > > const&) + 835\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x00000001a2b24e30 xgboost::ArrayInterface::Initialize(std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, xgboost::Json, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, xgboost::Json> > > const&, bool) + 48\n",
      "  [bt] (3) 4   libxgboost.dylib                    0x00000001a2b32d81 xgboost::data::ArrayAdapter::ArrayAdapter(xgboost::StringView) + 161\n",
      "  [bt] (4) 5   libxgboost.dylib                    0x00000001a2b321c4 XGBoosterPredictFromDense + 164\n",
      "  [bt] (5) 6   libffi.7.dylib                      0x0000000103127e9d ffi_call_unix64 + 85\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:55:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\t\t\tFold 2: Encountered an error.\n",
      "\t\t\tFold 2: All scores will be replaced with nan.\n",
      "\t\t\tFold 2: Please check the log file for the current hyperparameters and stack trace.\n",
      "\t\t\tFold 2: Exception during automl search: [14:55:39] ../src/c_api/../data/array_interface.h:139: Check failed: typestr.size() == 3 (2 vs. 3) : `typestr' should be of format <endian><type><size of type in bytes>.\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x00000001a2b2b074 dmlc::LogMessageFatal::~LogMessageFatal() + 116\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x00000001a2b25773 xgboost::ArrayInterfaceHandler::Validate(std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, xgboost::Json, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, xgboost::Json> > > const&) + 835\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x00000001a2b24e30 xgboost::ArrayInterface::Initialize(std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, xgboost::Json, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, xgboost::Json> > > const&, bool) + 48\n",
      "  [bt] (3) 4   libxgboost.dylib                    0x00000001a2b32d81 xgboost::data::ArrayAdapter::ArrayAdapter(xgboost::StringView) + 161\n",
      "  [bt] (4) 5   libxgboost.dylib                    0x00000001a2b321c4 XGBoosterPredictFromDense + 164\n",
      "  [bt] (5) 6   libffi.7.dylib                      0x0000000103127e9d ffi_call_unix64 + 85\n",
      "\n",
      "\n",
      "XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: nan\n",
      "Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.338\n",
      "CatBoost Classifier w/ Imputer + DateTime Featurization Component + SMOTENC Oversampler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.602\n",
      "\n",
      "Search finished after 00:19            \n",
      "Best pipeline: Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler\n",
      "Best pipeline Log Loss Binary: 0.285613\n"
     ]
    }
   ],
   "source": [
    "automl = evalml.automl.AutoMLSearch(X_train=X_train, y_train=y_train, problem_type='binary')\n",
    "automl.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoML search will log its progress, reporting each pipeline and parameter set evaluated during the search.\n",
    "\n",
    "There are a number of mechanisms to control the AutoML search time. One way is to set the `max_batches` parameter which controls the maximum number of rounds of AutoML to evaluate, where each round may train and score a variable number of pipelines. Another way is to set the `max_iterations` parameter which controls the maximum number of candidate models to be evaluated during AutoML. By default, AutoML will search for a single batch. The first pipeline to be evaluated will always be a baseline model representing a trivial solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoML interface supports a variety of other parameters. For a comprehensive list, please [refer to the API reference.](../autoapi/evalml/automl/index.rst#evalml.automl.AutoMLSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide [a standalone search method](../autoapi/evalml/automl/index.rst#evalml.automl.search) which does all of the above in a single line, and returns the `AutoMLSearch` instance and data check results. If there were data check errors, AutoML will not be run and no `AutoMLSearch` instance will be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Problem Type\n",
    "\n",
    "EvalML includes a simple method, `detect_problem_type`, to help determine the problem type given the target data. \n",
    "\n",
    "This function can return the predicted problem type as a ProblemType enum, choosing from ProblemType.BINARY, ProblemType.MULTICLASS, and ProblemType.REGRESSION. If the target data is invalid (for instance when there is only 1 unique label), the function will throw an error instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.258443Z",
     "start_time": "2021-07-27T18:55:43.254034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ProblemTypes.BINARY: 'binary'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evalml.problem_types import detect_problem_type\n",
    "\n",
    "y_binary = pd.Series([0, 1, 1, 0, 1, 1])\n",
    "detect_problem_type(y_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective parameter\n",
    "\n",
    "AutoMLSearch takes in an `objective` parameter to determine which `objective` to optimize for. By default, this parameter is set to `auto`, which allows AutoML to choose `LogLossBinary` for binary classification problems, `LogLossMulticlass` for multiclass classification problems, and `R2` for regression problems.\n",
    "\n",
    "It should be noted that the `objective` parameter is only used in ranking and helping choose the pipelines to iterate over, but is not used to optimize each individual pipeline during fit-time.\n",
    "\n",
    "To get the default objective for each problem type, you can use the `get_default_primary_search_objective` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.264214Z",
     "start_time": "2021-07-27T18:55:43.260027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss Binary\n",
      "Log Loss Multiclass\n",
      "R2\n"
     ]
    }
   ],
   "source": [
    "from evalml.automl import get_default_primary_search_objective\n",
    "\n",
    "binary_objective = get_default_primary_search_objective(\"binary\")\n",
    "multiclass_objective = get_default_primary_search_objective(\"multiclass\")\n",
    "regression_objective = get_default_primary_search_objective(\"regression\")\n",
    "\n",
    "print(binary_objective.name)\n",
    "print(multiclass_objective.name)\n",
    "print(regression_objective.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using custom pipelines\n",
    "\n",
    "EvalML's AutoML algorithm generates a set of pipelines to search with. To provide a custom set instead, set allowed_component_graphs to a dictionary of custom component graphs. `AutoMLSearch` will use these to generate `Pipeline` instances. Note: this will prevent AutoML from generating other pipelines to search over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.296933Z",
     "start_time": "2021-07-27T18:55:43.266834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default limit of max_batches=1.\n",
      "\n",
      "2 pipelines ready for search.\n"
     ]
    }
   ],
   "source": [
    "from evalml.pipelines import MulticlassClassificationPipeline\n",
    "\n",
    "\n",
    "automl_custom = evalml.automl.AutoMLSearch(X_train=X_train,\n",
    "                                           y_train=y_train,\n",
    "                                           problem_type='multiclass',\n",
    "                                           allowed_component_graphs={\"My_pipeline\": ['Simple Imputer', 'Random Forest Classifier'],\n",
    "                                                                     \"My_other_pipeline\": ['One Hot Encoder', 'Random Forest Classifier']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the search early\n",
    "\n",
    "To stop the search early, hit `Ctrl-C`. This will bring up a prompt asking for confirmation. Responding with `y` will immediately stop the search. Responding with `n` will continue the search.\n",
    "\n",
    "![Interrupting Search Demo](keyboard_interrupt_demo_updated.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback functions\n",
    "\n",
    "``AutoMLSearch`` supports several callback functions, which can be specified as parameters when initializing an ``AutoMLSearch`` object. They are:\n",
    "\n",
    "- ``start_iteration_callback``\n",
    "- ``add_result_callback``\n",
    "- ``error_callback``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Iteration Callback\n",
    "Users can set ``start_iteration_callback`` to set what function is called before each pipeline training iteration. This callback function must take three positional parameters: the pipeline class, the pipeline parameters, and the ``AutoMLSearch`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.301696Z",
     "start_time": "2021-07-27T18:55:43.298565Z"
    }
   },
   "outputs": [],
   "source": [
    "## start_iteration_callback example function\n",
    "def start_iteration_callback_example(pipeline_class, pipeline_params, automl_obj):\n",
    "    print (\"Training pipeline with the following parameters:\", pipeline_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Result Callback\n",
    "Users can set ``add_result_callback`` to set what function is called after each pipeline training iteration. This callback function must take three positional parameters: a dictionary containing the training results for the new pipeline, an untrained_pipeline containing the parameters used during training, and the ``AutoMLSearch`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.308747Z",
     "start_time": "2021-07-27T18:55:43.306333Z"
    }
   },
   "outputs": [],
   "source": [
    "## add_result_callback example function\n",
    "def add_result_callback_example(pipeline_results_dict, untrained_pipeline, automl_obj):\n",
    "    print (\"Results for trained pipeline with the following parameters:\", pipeline_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Callback\n",
    "Users can set the ``error_callback`` to set what function called when `search()` errors and raises an ``Exception``. This callback function takes three positional parameters: the ``Exception raised``, the traceback, and the ``AutoMLSearch object``. This callback function must also accept ``kwargs``, so ``AutoMLSearch`` is able to pass along other parameters used by default.\n",
    "\n",
    "Evalml defines several error callback functions, which can be found under `evalml.automl.callbacks`. They are:\n",
    "\n",
    "- `silent_error_callback`\n",
    "- `raise_error_callback`\n",
    "- `log_and_save_error_callback`\n",
    "- `raise_and_save_error_callback`\n",
    "- `log_error_callback` (default used when ``error_callback`` is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.314738Z",
     "start_time": "2021-07-27T18:55:43.310477Z"
    }
   },
   "outputs": [],
   "source": [
    "# error_callback example; this is implemented in the evalml library\n",
    "def raise_error_callback(exception, traceback, automl, **kwargs):\n",
    "    \"\"\"Raises the exception thrown by the AutoMLSearch object. Also logs the exception as an error.\"\"\"\n",
    "    logger.error(f'AutoMLSearch raised a fatal exception: {str(exception)}')\n",
    "    logger.error(\"\\n\".join(traceback))\n",
    "    raise exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Rankings\n",
    "A summary of all the pipelines built can be returned as a pandas DataFrame which is sorted by score. The score column contains the average score across all cross-validation folds while the validation_score column is computed from the first cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.351180Z",
     "start_time": "2021-07-27T18:55:43.317306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>search_order</th>\n",
       "      <th>mean_cv_score</th>\n",
       "      <th>standard_deviation_cv_score</th>\n",
       "      <th>validation_score</th>\n",
       "      <th>percent_better_than_baseline</th>\n",
       "      <th>high_variance_cv</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Random Forest Classifier w/ Imputer + DateTime...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.285613</td>\n",
       "      <td>0.041116</td>\n",
       "      <td>0.263695</td>\n",
       "      <td>92.806475</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>LightGBM Classifier w/ Imputer + DateTime Feat...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.308636</td>\n",
       "      <td>0.203878</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>92.226623</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Extra Trees Classifier w/ Imputer + DateTime F...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.338286</td>\n",
       "      <td>0.009381</td>\n",
       "      <td>0.329015</td>\n",
       "      <td>91.479857</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Elastic Net Classifier w/ Imputer + DateTime F...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.511808</td>\n",
       "      <td>0.074992</td>\n",
       "      <td>0.590517</td>\n",
       "      <td>87.109474</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Logistic Regression Classifier w/ Imputer + Da...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.551960</td>\n",
       "      <td>0.082695</td>\n",
       "      <td>0.628646</td>\n",
       "      <td>86.098206</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>CatBoost Classifier w/ Imputer + DateTime Feat...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.601819</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.593693</td>\n",
       "      <td>84.842444</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree Classifier w/ Imputer + DateTime...</td>\n",
       "      <td>2</td>\n",
       "      <td>2.956956</td>\n",
       "      <td>3.084439</td>\n",
       "      <td>0.651557</td>\n",
       "      <td>25.525413</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Mode Baseline Binary Classification Pipeline</td>\n",
       "      <td>0</td>\n",
       "      <td>3.970423</td>\n",
       "      <td>0.266060</td>\n",
       "      <td>4.124033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Baseline Classifier': {'strategy': 'mode'}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>XGBoost Classifier w/ Imputer + DateTime Featu...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      pipeline_name  search_order  \\\n",
       "0   3  Random Forest Classifier w/ Imputer + DateTime...             3   \n",
       "1   4  LightGBM Classifier w/ Imputer + DateTime Feat...             4   \n",
       "2   7  Extra Trees Classifier w/ Imputer + DateTime F...             7   \n",
       "3   1  Elastic Net Classifier w/ Imputer + DateTime F...             1   \n",
       "4   5  Logistic Regression Classifier w/ Imputer + Da...             5   \n",
       "5   8  CatBoost Classifier w/ Imputer + DateTime Feat...             8   \n",
       "6   2  Decision Tree Classifier w/ Imputer + DateTime...             2   \n",
       "7   0       Mode Baseline Binary Classification Pipeline             0   \n",
       "8   6  XGBoost Classifier w/ Imputer + DateTime Featu...             6   \n",
       "\n",
       "   mean_cv_score  standard_deviation_cv_score  validation_score  \\\n",
       "0       0.285613                     0.041116          0.263695   \n",
       "1       0.308636                     0.203878          0.234947   \n",
       "2       0.338286                     0.009381          0.329015   \n",
       "3       0.511808                     0.074992          0.590517   \n",
       "4       0.551960                     0.082695          0.628646   \n",
       "5       0.601819                     0.007246          0.593693   \n",
       "6       2.956956                     3.084439          0.651557   \n",
       "7       3.970423                     0.266060          4.124033   \n",
       "8            NaN                          NaN               NaN   \n",
       "\n",
       "   percent_better_than_baseline  high_variance_cv  \\\n",
       "0                     92.806475             False   \n",
       "1                     92.226623              True   \n",
       "2                     91.479857             False   \n",
       "3                     87.109474             False   \n",
       "4                     86.098206             False   \n",
       "5                     84.842444             False   \n",
       "6                     25.525413              True   \n",
       "7                      0.000000             False   \n",
       "8                           NaN             False   \n",
       "\n",
       "                                          parameters  \n",
       "0  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "1  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "2  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "3  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "4  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "5  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "6  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "7      {'Baseline Classifier': {'strategy': 'mode'}}  \n",
       "8  {'Imputer': {'categorical_impute_strategy': 'm...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Pipeline\n",
    "Each pipeline is given an `id`. We can get more information about any particular pipeline using that `id`. Here, we will get more information about the pipeline with `id = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.411881Z",
     "start_time": "2021-07-27T18:55:43.352731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************************************************************************\n",
      "* Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler + Standard Scaler *\n",
      "**********************************************************************************************************************************\n",
      "\n",
      "Problem Type: binary\n",
      "Model Family: Linear\n",
      "\n",
      "Pipeline Steps\n",
      "==============\n",
      "1. Imputer\n",
      "\t * categorical_impute_strategy : most_frequent\n",
      "\t * numeric_impute_strategy : mean\n",
      "\t * categorical_fill_value : None\n",
      "\t * numeric_fill_value : None\n",
      "2. DateTime Featurization Component\n",
      "\t * features_to_extract : ['year', 'month', 'day_of_week', 'hour']\n",
      "\t * encode_as_categories : False\n",
      "\t * date_index : None\n",
      "3. One Hot Encoder\n",
      "\t * top_n : 10\n",
      "\t * features_to_encode : None\n",
      "\t * categories : None\n",
      "\t * drop : if_binary\n",
      "\t * handle_unknown : ignore\n",
      "\t * handle_missing : error\n",
      "4. SMOTENC Oversampler\n",
      "\t * sampling_ratio : 0.25\n",
      "\t * k_neighbors_default : 5\n",
      "\t * n_jobs : -1\n",
      "\t * sampling_ratio_dict : None\n",
      "\t * categorical_features : [3, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
      "\t * k_neighbors : 5\n",
      "5. Standard Scaler\n",
      "6. Elastic Net Classifier\n",
      "\t * penalty : elasticnet\n",
      "\t * C : 1.0\n",
      "\t * l1_ratio : 0.15\n",
      "\t * n_jobs : -1\n",
      "\t * multi_class : auto\n",
      "\t * solver : saga\n",
      "\n",
      "Training\n",
      "========\n",
      "Training for binary problems.\n",
      "Total training time (including CV): 2.2 seconds\n",
      "\n",
      "Cross Validation\n",
      "----------------\n",
      "             Log Loss Binary  MCC Binary  Gini   AUC  Precision    F1  Balanced Accuracy Binary  Accuracy Binary # Training # Validation\n",
      "0                      0.591       0.175 0.195 0.597      0.286 0.267                     0.583            0.836        133           67\n",
      "1                      0.441       0.296 0.419 0.710      0.500 0.333                     0.608            0.881        133           67\n",
      "2                      0.504       0.035 0.259 0.630      0.120 0.188                     0.528            0.606        134           66\n",
      "mean                   0.512       0.169 0.291 0.646      0.302 0.263                     0.573            0.774          -            -\n",
      "std                    0.075       0.130 0.116 0.058      0.191 0.073                     0.041            0.147          -            -\n",
      "coef of var            0.147       0.772 0.397 0.090      0.631 0.278                     0.072            0.190          -            -\n"
     ]
    }
   ],
   "source": [
    "automl.describe_pipeline(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pipeline\n",
    "We can get the object of any pipeline via their `id` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.420462Z",
     "start_time": "2021-07-27T18:55:43.413731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler + Standard Scaler\n",
      "{'Imputer': {'categorical_impute_strategy': 'most_frequent', 'numeric_impute_strategy': 'mean', 'categorical_fill_value': None, 'numeric_fill_value': None}, 'DateTime Featurization Component': {'features_to_extract': ['year', 'month', 'day_of_week', 'hour'], 'encode_as_categories': False, 'date_index': None}, 'One Hot Encoder': {'top_n': 10, 'features_to_encode': None, 'categories': None, 'drop': 'if_binary', 'handle_unknown': 'ignore', 'handle_missing': 'error'}, 'SMOTENC Oversampler': {'sampling_ratio': 0.25, 'k_neighbors_default': 5, 'n_jobs': -1, 'sampling_ratio_dict': None, 'categorical_features': [3, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], 'k_neighbors': 5}, 'Elastic Net Classifier': {'penalty': 'elasticnet', 'C': 1.0, 'l1_ratio': 0.15, 'n_jobs': -1, 'multi_class': 'auto', 'solver': 'saga'}}\n"
     ]
    }
   ],
   "source": [
    "pipeline = automl.get_pipeline(1)\n",
    "print(pipeline.name)\n",
    "print(pipeline.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best pipeline\n",
    "If you specifically want to get the best pipeline, there is a convenient accessor for that.\n",
    "The pipeline returned is already fitted on the input X, y data that we passed to AutoMLSearch. To turn off this default behavior, set `train_best_pipeline=False` when initializing AutoMLSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:43.596746Z",
     "start_time": "2021-07-27T18:55:43.422357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler\n",
      "{'Imputer': {'categorical_impute_strategy': 'most_frequent', 'numeric_impute_strategy': 'mean', 'categorical_fill_value': None, 'numeric_fill_value': None}, 'DateTime Featurization Component': {'features_to_extract': ['year', 'month', 'day_of_week', 'hour'], 'encode_as_categories': False, 'date_index': None}, 'One Hot Encoder': {'top_n': 10, 'features_to_encode': None, 'categories': None, 'drop': 'if_binary', 'handle_unknown': 'ignore', 'handle_missing': 'error'}, 'SMOTENC Oversampler': {'sampling_ratio': 0.25, 'k_neighbors_default': 5, 'n_jobs': -1, 'sampling_ratio_dict': None, 'categorical_features': [3, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], 'k_neighbors': 5}, 'Random Forest Classifier': {'n_estimators': 100, 'max_depth': 6, 'n_jobs': -1}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "195    False\n",
       "196    False\n",
       "197    False\n",
       "198    False\n",
       "199    False\n",
       "Name: fraud, Length: 200, dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline = automl.best_pipeline\n",
    "print(best_pipeline.name)\n",
    "print(best_pipeline.parameters)\n",
    "best_pipeline.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Scoring Multiple Pipelines using AutoMLSearch\n",
    "\n",
    "AutoMLSearch will automatically fit the best pipeline on the entire training data. It also provides an easy API for training and scoring other pipelines.\n",
    "\n",
    "If you'd like to train one or more pipelines on the entire training data, you can use the `train_pipelines`method\n",
    "\n",
    "Similarly, if you'd like to score one or more pipelines on a particular dataset, you can use the `train_pipelines`method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:45.052030Z",
     "start_time": "2021-07-27T18:55:43.599021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mode Baseline Binary Classification Pipeline': pipeline = BinaryClassificationPipeline(component_graph={'Baseline Classifier': ['Baseline Classifier', 'X', 'y']}, parameters={'Baseline Classifier':{'strategy': 'mode'}}, custom_name='Mode Baseline Binary Classification Pipeline', random_seed=0),\n",
       " 'Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler + Standard Scaler': pipeline = BinaryClassificationPipeline(component_graph={'Imputer': ['Imputer', 'X', 'y'], 'DateTime Featurization Component': ['DateTime Featurization Component', 'Imputer.x', 'y'], 'One Hot Encoder': ['One Hot Encoder', 'DateTime Featurization Component.x', 'y'], 'SMOTENC Oversampler': ['SMOTENC Oversampler', 'One Hot Encoder.x', 'y'], 'Standard Scaler': ['Standard Scaler', 'SMOTENC Oversampler.x', 'SMOTENC Oversampler.y'], 'Elastic Net Classifier': ['Elastic Net Classifier', 'Standard Scaler.x', 'SMOTENC Oversampler.y']}, parameters={'Imputer':{'categorical_impute_strategy': 'most_frequent', 'numeric_impute_strategy': 'mean', 'categorical_fill_value': None, 'numeric_fill_value': None}, 'DateTime Featurization Component':{'features_to_extract': ['year', 'month', 'day_of_week', 'hour'], 'encode_as_categories': False, 'date_index': None}, 'One Hot Encoder':{'top_n': 10, 'features_to_encode': None, 'categories': None, 'drop': 'if_binary', 'handle_unknown': 'ignore', 'handle_missing': 'error'}, 'SMOTENC Oversampler':{'sampling_ratio': 0.25, 'k_neighbors_default': 5, 'n_jobs': -1, 'sampling_ratio_dict': None, 'categorical_features': [3, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], 'k_neighbors': 5}, 'Elastic Net Classifier':{'penalty': 'elasticnet', 'C': 1.0, 'l1_ratio': 0.15, 'n_jobs': -1, 'multi_class': 'auto', 'solver': 'saga'}}, random_seed=0),\n",
       " 'Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler': pipeline = BinaryClassificationPipeline(component_graph={'Imputer': ['Imputer', 'X', 'y'], 'DateTime Featurization Component': ['DateTime Featurization Component', 'Imputer.x', 'y'], 'One Hot Encoder': ['One Hot Encoder', 'DateTime Featurization Component.x', 'y'], 'SMOTENC Oversampler': ['SMOTENC Oversampler', 'One Hot Encoder.x', 'y'], 'Decision Tree Classifier': ['Decision Tree Classifier', 'SMOTENC Oversampler.x', 'SMOTENC Oversampler.y']}, parameters={'Imputer':{'categorical_impute_strategy': 'most_frequent', 'numeric_impute_strategy': 'mean', 'categorical_fill_value': None, 'numeric_fill_value': None}, 'DateTime Featurization Component':{'features_to_extract': ['year', 'month', 'day_of_week', 'hour'], 'encode_as_categories': False, 'date_index': None}, 'One Hot Encoder':{'top_n': 10, 'features_to_encode': None, 'categories': None, 'drop': 'if_binary', 'handle_unknown': 'ignore', 'handle_missing': 'error'}, 'SMOTENC Oversampler':{'sampling_ratio': 0.25, 'k_neighbors_default': 5, 'n_jobs': -1, 'sampling_ratio_dict': None, 'categorical_features': [3, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], 'k_neighbors': 5}, 'Decision Tree Classifier':{'criterion': 'gini', 'max_features': 'auto', 'max_depth': 6, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0}}, random_seed=0)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_pipelines = automl.train_pipelines([automl.get_pipeline(i) for i in [0, 1, 2]])\n",
    "trained_pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:45.324747Z",
     "start_time": "2021-07-27T18:55:45.053603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mode Baseline Binary Classification Pipeline': OrderedDict([('Accuracy Binary',\n",
       "               0.88),\n",
       "              ('F1', 0.0),\n",
       "              ('AUC', 0.5)]),\n",
       " 'Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler + Standard Scaler': OrderedDict([('Accuracy Binary',\n",
       "               0.66),\n",
       "              ('F1', 0.19047619047619044),\n",
       "              ('AUC', 0.5265151515151515)]),\n",
       " 'Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler': OrderedDict([('Accuracy Binary',\n",
       "               0.92),\n",
       "              ('F1', 0.6),\n",
       "              ('AUC', 0.7234848484848486)])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_holdout_scores = automl.score_pipelines([trained_pipelines[name] for name in trained_pipelines.keys()],\n",
    "                                                X_holdout,\n",
    "                                                y_holdout,\n",
    "                                                ['Accuracy Binary', 'F1', 'AUC'])\n",
    "pipeline_holdout_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving AutoMLSearch and pipelines from AutoMLSearch\n",
    "\n",
    "There are two ways to save results from AutoMLSearch. \n",
    "\n",
    "- You can save the AutoMLSearch object itself, calling `.save(<filepath>)` to do so. This will allow you to save the AutoMLSearch state and reload all pipelines from this.\n",
    "\n",
    "- If you want to save a pipeline from AutoMLSearch for future use, pipeline classes themselves have a `.save(<filepath>)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:45.577108Z",
     "start_time": "2021-07-27T18:55:45.325991Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving the entire automl search\n",
    "automl.save(\"automl.cloudpickle\")\n",
    "automl2 = evalml.automl.AutoMLSearch.load(\"automl.cloudpickle\")\n",
    "# saving the best pipeline using .save()\n",
    "best_pipeline.save(\"pipeline.cloudpickle\")\n",
    "best_pipeline_copy = evalml.pipelines.PipelineBase.load(\"pipeline.cloudpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting the AutoML Search Space\n",
    "The AutoML search algorithm first trains each component in the pipeline with their default values. After the first iteration, it then tweaks the parameters of these components using the pre-defined hyperparameter ranges that these components have. To limit the search over certain hyperparameter ranges, you can specify a `custom_hyperparameters` argument with your `AutoMLSearch` parameters. These parameters will limit the hyperparameter search space. \n",
    "\n",
    "Hyperparameter ranges can be found through the [API reference](https://evalml.alteryx.com/en/stable/api_reference.html) for each component. Parameter arguments must be specified as dictionaries, but the associated values can be single values or `skopt.space` Real, Integer, Categorical values.\n",
    "\n",
    "If however you'd like to specify certain values for the initial batch of the AutoML search algorithm, you can use the `pipeline_parameters` argument. This will set the initial batch's component parameters to the values passed by this argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:46.463460Z",
     "start_time": "2021-07-27T18:55:45.579283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 1000\n",
      "Targets\n",
      "False    85.90%\n",
      "True     14.10%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "8 pipelines ready for search.\n"
     ]
    }
   ],
   "source": [
    "from evalml import AutoMLSearch\n",
    "from evalml.demos import load_fraud\n",
    "from skopt.space import Categorical\n",
    "from evalml.model_family import ModelFamily\n",
    "import woodwork as ww\n",
    "\n",
    "X, y = load_fraud(n_rows=1000)\n",
    "\n",
    "# example of setting parameter to just one value\n",
    "custom_hyperparameters = {'Imputer': {\n",
    "    'numeric_impute_strategy': 'mean'\n",
    "}}\n",
    "\n",
    "\n",
    "# limit the numeric impute strategy to include only `median` and `most_frequent`\n",
    "# `mean` is the default value for this argument, but it doesn't need to be included in the specified hyperparameter range for this to work\n",
    "custom_hyperparameters = {'Imputer': {\n",
    "    'numeric_impute_strategy': Categorical(['median', 'most_frequent'])\n",
    "}}\n",
    "# set the initial batch numeric impute strategy strategy to 'median'\n",
    "pipeline_parameters = {'Imputer': {\n",
    "    'numeric_impute_strategy': 'median'\n",
    "}}\n",
    "\n",
    "# using this custom hyperparameter means that our Imputer components in these pipelines will only search through\n",
    "# 'median' and 'most_frequent' strategies for 'numeric_impute_strategy', and the initial batch parameter will be\n",
    "# set to 'median'\n",
    "automl_constrained = AutoMLSearch(X_train=X, y_train=y, problem_type='binary', pipeline_parameters=pipeline_parameters,\n",
    "                                  custom_hyperparameters=custom_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Data\n",
    "The AutoML search algorithm now has functionality to handle imbalanced data during classification! AutoMLSearch now provides two additional parameters, `sampler_method` and `sampler_balanced_ratio`, that allow you to let AutoMLSearch know whether to sample imbalanced data, and how to do so. `sampler_method` takes in either `Undersampler`, `Oversampler`, `auto`, or None as the sampler to use, and `sampler_balanced_ratio` specifies the `minority/majority` ratio that you want to sample to. Details on the Undersampler and Oversampler components can be found in the [documentation](https://evalml.alteryx.com/en/stable/api_reference.html#transformers).\n",
    "\n",
    "This can be used for imbalanced datasets, like the fraud dataset, which has a 'minority:majority' ratio of < 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:46.556999Z",
     "start_time": "2021-07-27T18:55:46.465147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "8 pipelines ready for search.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pipeline = BinaryClassificationPipeline(component_graph={'Imputer': ['Imputer', 'X', 'y'], 'DateTime Featurization Component': ['DateTime Featurization Component', 'Imputer.x', 'y'], 'One Hot Encoder': ['One Hot Encoder', 'DateTime Featurization Component.x', 'y'], 'SMOTENC Oversampler': ['SMOTENC Oversampler', 'One Hot Encoder.x', 'y'], 'Standard Scaler': ['Standard Scaler', 'SMOTENC Oversampler.x', 'SMOTENC Oversampler.y'], 'Logistic Regression Classifier': ['Logistic Regression Classifier', 'Standard Scaler.x', 'SMOTENC Oversampler.y']}, parameters={'Imputer':{'categorical_impute_strategy': 'most_frequent', 'numeric_impute_strategy': 'mean', 'categorical_fill_value': None, 'numeric_fill_value': None}, 'DateTime Featurization Component':{'features_to_extract': ['year', 'month', 'day_of_week', 'hour'], 'encode_as_categories': False, 'date_index': None}, 'One Hot Encoder':{'top_n': 10, 'features_to_encode': None, 'categories': None, 'drop': 'if_binary', 'handle_unknown': 'ignore', 'handle_missing': 'error'}, 'SMOTENC Oversampler':{'sampling_ratio': 0.25, 'k_neighbors_default': 5, 'n_jobs': -1, 'sampling_ratio_dict': None}, 'Logistic Regression Classifier':{'penalty': 'l2', 'C': 1.0, 'n_jobs': -1, 'multi_class': 'auto', 'solver': 'lbfgs'}}, random_seed=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl_auto = AutoMLSearch(X_train=X, y_train=y, problem_type='binary')\n",
    "automl_auto.allowed_pipelines[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SMOTENC Oversampler is chosen as the default sampling component here, since the `sampler_balanced_ratio = 0.25`. If you specified a lower ratio, for instance `sampler_balanced_ratio = 0.1`, then there would be no sampling component added here. This is because if a ratio of 0.1 would be considered balanced, then a ratio of 0.2 would also be balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:46.646648Z",
     "start_time": "2021-07-27T18:55:46.558495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "8 pipelines ready for search.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pipeline = BinaryClassificationPipeline(component_graph={'Imputer': ['Imputer', 'X', 'y'], 'DateTime Featurization Component': ['DateTime Featurization Component', 'Imputer.x', 'y'], 'One Hot Encoder': ['One Hot Encoder', 'DateTime Featurization Component.x', 'y'], 'Standard Scaler': ['Standard Scaler', 'One Hot Encoder.x', 'y'], 'Logistic Regression Classifier': ['Logistic Regression Classifier', 'Standard Scaler.x', 'y']}, parameters={'Imputer':{'categorical_impute_strategy': 'most_frequent', 'numeric_impute_strategy': 'mean', 'categorical_fill_value': None, 'numeric_fill_value': None}, 'DateTime Featurization Component':{'features_to_extract': ['year', 'month', 'day_of_week', 'hour'], 'encode_as_categories': False, 'date_index': None}, 'One Hot Encoder':{'top_n': 10, 'features_to_encode': None, 'categories': None, 'drop': 'if_binary', 'handle_unknown': 'ignore', 'handle_missing': 'error'}, 'Logistic Regression Classifier':{'penalty': 'l2', 'C': 1.0, 'n_jobs': -1, 'multi_class': 'auto', 'solver': 'lbfgs'}}, random_seed=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl_auto_ratio = AutoMLSearch(X_train=X, y_train=y, problem_type='binary', sampler_balanced_ratio=0.1)\n",
    "automl_auto_ratio.allowed_pipelines[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can add more fine-grained sampling ratios by passing in a `sampling_ratio_dict` in pipeline parameters. For this dictionary, AutoMLSearch expects the keys to be int values from 0 to `n-1` for the classes, and the values would be the `sampler_balanced__ratio` associated with each target. This dictionary would override the AutoML argument `sampler_balanced_ratio`. Below, you can see the scenario for Oversampler component on this dataset. Note that the logic for Undersamplers is included in the commented section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:55:46.740978Z",
     "start_time": "2021-07-27T18:55:46.648103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "8 pipelines ready for search.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pipeline = BinaryClassificationPipeline(component_graph={'Imputer': ['Imputer', 'X', 'y'], 'DateTime Featurization Component': ['DateTime Featurization Component', 'Imputer.x', 'y'], 'One Hot Encoder': ['One Hot Encoder', 'DateTime Featurization Component.x', 'y'], 'SMOTENC Oversampler': ['SMOTENC Oversampler', 'One Hot Encoder.x', 'y'], 'Standard Scaler': ['Standard Scaler', 'SMOTENC Oversampler.x', 'SMOTENC Oversampler.y'], 'Logistic Regression Classifier': ['Logistic Regression Classifier', 'Standard Scaler.x', 'SMOTENC Oversampler.y']}, parameters={'Imputer':{'categorical_impute_strategy': 'most_frequent', 'numeric_impute_strategy': 'mean', 'categorical_fill_value': None, 'numeric_fill_value': None}, 'DateTime Featurization Component':{'features_to_extract': ['year', 'month', 'day_of_week', 'hour'], 'encode_as_categories': False, 'date_index': None}, 'One Hot Encoder':{'top_n': 10, 'features_to_encode': None, 'categories': None, 'drop': 'if_binary', 'handle_unknown': 'ignore', 'handle_missing': 'error'}, 'SMOTENC Oversampler':{'sampling_ratio': 0.25, 'k_neighbors_default': 5, 'n_jobs': -1, 'sampling_ratio_dict': None, 'sampler_balanced_ratio': {0: 1, 1: 0.5}}, 'Logistic Regression Classifier':{'penalty': 'l2', 'C': 1.0, 'n_jobs': -1, 'multi_class': 'auto', 'solver': 'lbfgs'}}, random_seed=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this case, the majority class is the negative class\n",
    "# for the oversampler, we don't want to oversample this class, so class 0 (majority) will have a ratio of 1 to itself\n",
    "# for the minority class 1, we want to oversample it to have a minority/majority ratio of 0.5, which means we want minority to have 1/2 the samples as the minority\n",
    "sampler_ratio_dict = {0: 1, 1: 0.5}\n",
    "pipeline_parameters = {\"SMOTENC Oversampler\": {\"sampler_balanced_ratio\": sampler_ratio_dict}}\n",
    "automl_auto_ratio_dict = AutoMLSearch(X_train=X, y_train=y, problem_type='binary', pipeline_parameters=pipeline_parameters)\n",
    "automl_auto_ratio_dict.allowed_pipelines[-1]\n",
    "\n",
    "# Undersampler case\n",
    "# we don't want to undersample this class, so class 1 (minority) will have a ratio of 1 to itself\n",
    "# for the majority class 0, we want to undersample it to have a minority/majority ratio of 0.5, which means we want majority to have 2x the samples as the minority\n",
    "# sampler_ratio_dict = {0: 0.5, 1: 1}\n",
    "# pipeline_parameters = {\"SMOTENC Oversampler\": {\"sampler_balanced_ratio\": sampler_ratio_dict}}\n",
    "# automl_auto_ratio_dict = AutoMLSearch(X_train=X, y_train=y, problem_type='binary', pipeline_parameters=pipeline_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding ensemble methods to AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "[Stacking](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking) is an ensemble machine learning algorithm that involves training a model to best combine the predictions of several base learning algorithms. First, each base learning algorithms is trained using the given data. Then, the combining algorithm or meta-learner is trained on the predictions made by those base learning algorithms to make a final prediction.\n",
    "\n",
    "AutoML enables stacking using the `ensembling` flag during initalization; this is set to `False` by default. The stacking ensemble pipeline runs in its own batch after a whole cycle of training has occurred (each allowed pipeline trains for one batch). Note that this means __a large number of iterations may need to run before the stacking ensemble runs__. It is also important to note that __only the first CV fold is calculated for stacking ensembles__ because the model internally uses CV folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:56:15.121991Z",
     "start_time": "2021-07-27T18:55:46.742187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Number of Features\n",
      "Numeric                  30\n",
      "\n",
      "Number of training examples: 569\n",
      "Targets\n",
      "benign       62.74%\n",
      "malignant    37.26%\n",
      "Name: target, dtype: object\n",
      "Generating pipelines to search over...\n",
      "2 pipelines ready for search.\n",
      "Ensembling will run every 3 batches.\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using SequentialEngine to train and score pipelines.\n",
      "Searching up to 4 batches for a total of 14 pipelines. \n",
      "Allowed model families: linear_model\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78583a84ac54162b624d7a4cb8be9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Best Score',\n",
       "              'type'\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 12.868\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Elastic Net Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.077\n",
      "Logistic Regression Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.077\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLogistic Regression Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 2 *\n",
      "*****************************\n",
      "\n",
      "Logistic Regression Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.097\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLogistic Regression Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "Logistic Regression Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.085\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLogistic Regression Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "Logistic Regression Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.097\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLogistic Regression Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "Logistic Regression Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.091\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLogistic Regression Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "Logistic Regression Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.080\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 3 *\n",
      "*****************************\n",
      "\n",
      "Elastic Net Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.075\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tElastic Net Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "Elastic Net Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.075\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tElastic Net Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "Elastic Net Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.079\n",
      "Elastic Net Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.076\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tElastic Net Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "Elastic Net Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.075\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tElastic Net Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 4 *\n",
      "*****************************\n",
      "\n",
      "Stacked Ensemble Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.123\n",
      "\n",
      "Search finished after 00:27            \n",
      "Best pipeline: Elastic Net Classifier w/ Imputer + Standard Scaler\n",
      "Best pipeline Log Loss Binary: 0.075387\n"
     ]
    }
   ],
   "source": [
    "X, y = evalml.demos.load_breast_cancer()\n",
    "\n",
    "automl_with_ensembling = AutoMLSearch(X_train=X, y_train=y,\n",
    "                                      problem_type=\"binary\",\n",
    "                                      allowed_model_families=[ModelFamily.LINEAR_MODEL],\n",
    "                                      max_batches=4,\n",
    "                                      ensembling=True)\n",
    "automl_with_ensembling.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view more information about the stacking ensemble pipeline (which was the best performing pipeline) by calling `.describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:56:15.137804Z",
     "start_time": "2021-07-27T18:56:15.123459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******************************************************\n",
      "* Elastic Net Classifier w/ Imputer + Standard Scaler *\n",
      "*******************************************************\n",
      "\n",
      "Problem Type: binary\n",
      "Model Family: Linear\n",
      "Number of features: 30\n",
      "\n",
      "Pipeline Steps\n",
      "==============\n",
      "1. Imputer\n",
      "\t * categorical_impute_strategy : most_frequent\n",
      "\t * numeric_impute_strategy : median\n",
      "\t * categorical_fill_value : None\n",
      "\t * numeric_fill_value : None\n",
      "2. Standard Scaler\n",
      "3. Elastic Net Classifier\n",
      "\t * penalty : elasticnet\n",
      "\t * C : 8.123565600467177\n",
      "\t * l1_ratio : 0.47997717237505744\n",
      "\t * n_jobs : -1\n",
      "\t * multi_class : auto\n",
      "\t * solver : saga\n"
     ]
    }
   ],
   "source": [
    "automl_with_ensembling.best_pipeline.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access raw results\n",
    "\n",
    "The `AutoMLSearch` class records detailed results information under the `results` field, including information about the cross-validation scoring and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:56:15.171015Z",
     "start_time": "2021-07-27T18:56:15.139988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline_results': {0: {'id': 0,\n",
       "   'pipeline_name': 'Mode Baseline Binary Classification Pipeline',\n",
       "   'pipeline_class': evalml.pipelines.binary_classification_pipeline.BinaryClassificationPipeline,\n",
       "   'pipeline_summary': 'Baseline Classifier',\n",
       "   'parameters': {'Baseline Classifier': {'strategy': 'mode'}},\n",
       "   'mean_cv_score': 3.970423187263591,\n",
       "   'standard_deviation_cv_score': 0.26606000431837074,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 0.8083591461181641,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   4.124033002377396),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('Gini', 0.0),\n",
       "                  ('AUC', 0.5),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.8805970149253731),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 4.124033002377396,\n",
       "     'binary_classification_threshold': 9.16384630183206e-53},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   4.124033002377395),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('Gini', 0.0),\n",
       "                  ('AUC', 0.5),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.8805970149253731),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 4.124033002377395,\n",
       "     'binary_classification_threshold': 9.16384630183206e-53},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   3.6632035570359824),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('Gini', 0.0),\n",
       "                  ('AUC', 0.5),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.8939393939393939),\n",
       "                  ('# Training', 134),\n",
       "                  ('# Validation', 66)]),\n",
       "     'mean_cv_score': 3.6632035570359824,\n",
       "     'binary_classification_threshold': 9.16384630183206e-53}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 0,\n",
       "    'MCC Binary': 0,\n",
       "    'Gini': 0,\n",
       "    'AUC': 0,\n",
       "    'Precision': 0,\n",
       "    'F1': 0,\n",
       "    'Balanced Accuracy Binary': 0,\n",
       "    'Accuracy Binary': 0},\n",
       "   'percent_better_than_baseline': 0,\n",
       "   'validation_score': 4.124033002377396},\n",
       "  1: {'id': 1,\n",
       "   'pipeline_name': 'Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler + Standard Scaler',\n",
       "   'pipeline_class': evalml.pipelines.binary_classification_pipeline.BinaryClassificationPipeline,\n",
       "   'pipeline_summary': 'Elastic Net Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler + Standard Scaler',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour'],\n",
       "     'encode_as_categories': False,\n",
       "     'date_index': None},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': 'if_binary',\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'SMOTENC Oversampler': {'sampling_ratio': 0.25,\n",
       "     'k_neighbors_default': 5,\n",
       "     'n_jobs': -1,\n",
       "     'sampling_ratio_dict': None,\n",
       "     'categorical_features': [3,\n",
       "      10,\n",
       "      11,\n",
       "      12,\n",
       "      13,\n",
       "      14,\n",
       "      15,\n",
       "      16,\n",
       "      17,\n",
       "      18,\n",
       "      19,\n",
       "      20,\n",
       "      21,\n",
       "      22,\n",
       "      23,\n",
       "      24,\n",
       "      25,\n",
       "      26,\n",
       "      27,\n",
       "      28,\n",
       "      29,\n",
       "      30,\n",
       "      31,\n",
       "      32,\n",
       "      33,\n",
       "      34,\n",
       "      35,\n",
       "      36,\n",
       "      37,\n",
       "      38,\n",
       "      39,\n",
       "      40,\n",
       "      41,\n",
       "      42,\n",
       "      43,\n",
       "      44,\n",
       "      45,\n",
       "      46,\n",
       "      47,\n",
       "      48,\n",
       "      49,\n",
       "      50,\n",
       "      51,\n",
       "      52,\n",
       "      53,\n",
       "      54,\n",
       "      55,\n",
       "      56,\n",
       "      57,\n",
       "      58,\n",
       "      59],\n",
       "     'k_neighbors': 5},\n",
       "    'Elastic Net Classifier': {'penalty': 'elasticnet',\n",
       "     'C': 1.0,\n",
       "     'l1_ratio': 0.15,\n",
       "     'n_jobs': -1,\n",
       "     'multi_class': 'auto',\n",
       "     'solver': 'saga'}},\n",
       "   'mean_cv_score': 0.511808422655102,\n",
       "   'standard_deviation_cv_score': 0.07499222153032734,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 2.1611690521240234,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.5905169143750514),\n",
       "                  ('MCC Binary', 0.17518582316850065),\n",
       "                  ('Gini', 0.19491525423728806),\n",
       "                  ('AUC', 0.597457627118644),\n",
       "                  ('Precision', 0.2857142857142857),\n",
       "                  ('F1', 0.26666666666666666),\n",
       "                  ('Balanced Accuracy Binary', 0.5826271186440678),\n",
       "                  ('Accuracy Binary', 0.835820895522388),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.5905169143750514,\n",
       "     'binary_classification_threshold': 0.48089822234602875},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.44118816828058094),\n",
       "                  ('MCC Binary', 0.2957528252716689),\n",
       "                  ('Gini', 0.4194915254237288),\n",
       "                  ('AUC', 0.7097457627118644),\n",
       "                  ('Precision', 0.5),\n",
       "                  ('F1', 0.3333333333333333),\n",
       "                  ('Balanced Accuracy Binary', 0.6080508474576272),\n",
       "                  ('Accuracy Binary', 0.8805970149253731),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.44118816828058094,\n",
       "     'binary_classification_threshold': 0.7518116371406225},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.5037201853096739),\n",
       "                  ('MCC Binary', 0.035350118786872956),\n",
       "                  ('Gini', 0.2590799031476996),\n",
       "                  ('AUC', 0.6295399515738498),\n",
       "                  ('Precision', 0.12),\n",
       "                  ('F1', 0.1875),\n",
       "                  ('Balanced Accuracy Binary', 0.5278450363196125),\n",
       "                  ('Accuracy Binary', 0.6060606060606061),\n",
       "                  ('# Training', 134),\n",
       "                  ('# Validation', 66)]),\n",
       "     'mean_cv_score': 0.5037201853096739,\n",
       "     'binary_classification_threshold': 0.10568708806540947}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 87.10947426720426,\n",
       "    'MCC Binary': inf,\n",
       "    'Gini': inf,\n",
       "    'AUC': 14.558111380145277,\n",
       "    'Precision': 30.19047619047619,\n",
       "    'F1': 26.25,\n",
       "    'Balanced Accuracy Binary': 7.284100080710254,\n",
       "    'Accuracy Binary': -11.08849690939243},\n",
       "   'percent_better_than_baseline': 87.10947426720426,\n",
       "   'validation_score': 0.5905169143750514},\n",
       "  2: {'id': 2,\n",
       "   'pipeline_name': 'Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler',\n",
       "   'pipeline_class': evalml.pipelines.binary_classification_pipeline.BinaryClassificationPipeline,\n",
       "   'pipeline_summary': 'Decision Tree Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour'],\n",
       "     'encode_as_categories': False,\n",
       "     'date_index': None},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': 'if_binary',\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'SMOTENC Oversampler': {'sampling_ratio': 0.25,\n",
       "     'k_neighbors_default': 5,\n",
       "     'n_jobs': -1,\n",
       "     'sampling_ratio_dict': None,\n",
       "     'categorical_features': [3,\n",
       "      10,\n",
       "      11,\n",
       "      12,\n",
       "      13,\n",
       "      14,\n",
       "      15,\n",
       "      16,\n",
       "      17,\n",
       "      18,\n",
       "      19,\n",
       "      20,\n",
       "      21,\n",
       "      22,\n",
       "      23,\n",
       "      24,\n",
       "      25,\n",
       "      26,\n",
       "      27,\n",
       "      28,\n",
       "      29,\n",
       "      30,\n",
       "      31,\n",
       "      32,\n",
       "      33,\n",
       "      34,\n",
       "      35,\n",
       "      36,\n",
       "      37,\n",
       "      38,\n",
       "      39,\n",
       "      40,\n",
       "      41,\n",
       "      42,\n",
       "      43,\n",
       "      44,\n",
       "      45,\n",
       "      46,\n",
       "      47,\n",
       "      48,\n",
       "      49,\n",
       "      50,\n",
       "      51,\n",
       "      52,\n",
       "      53,\n",
       "      54,\n",
       "      55,\n",
       "      56,\n",
       "      57,\n",
       "      58,\n",
       "      59],\n",
       "     'k_neighbors': 5},\n",
       "    'Decision Tree Classifier': {'criterion': 'gini',\n",
       "     'max_features': 'auto',\n",
       "     'max_depth': 6,\n",
       "     'min_samples_split': 2,\n",
       "     'min_weight_fraction_leaf': 0.0}},\n",
       "   'mean_cv_score': 2.956956288246977,\n",
       "   'standard_deviation_cv_score': 3.084438905133218,\n",
       "   'high_variance_cv': True,\n",
       "   'training_time': 1.6705949306488037,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.651556943513563),\n",
       "                  ('MCC Binary', 0.7712055916006633),\n",
       "                  ('Gini', 0.6207627118644068),\n",
       "                  ('AUC', 0.8103813559322034),\n",
       "                  ('Precision', 1.0),\n",
       "                  ('F1', 0.7692307692307693),\n",
       "                  ('Balanced Accuracy Binary', 0.8125),\n",
       "                  ('Accuracy Binary', 0.9552238805970149),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.651556943513563,\n",
       "     'binary_classification_threshold': 0.9999999885588493},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   1.7585681792927779),\n",
       "                  ('MCC Binary', 0.4208952550769721),\n",
       "                  ('Gini', 0.379237288135593),\n",
       "                  ('AUC', 0.6896186440677965),\n",
       "                  ('Precision', 0.6),\n",
       "                  ('F1', 0.4615384615384615),\n",
       "                  ('Balanced Accuracy Binary', 0.6705508474576272),\n",
       "                  ('Accuracy Binary', 0.8955223880597015),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 1.7585681792927779,\n",
       "     'binary_classification_threshold': 0.9999999885588493},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   6.460743741934591),\n",
       "                  ('MCC Binary', -0.20116515012026231),\n",
       "                  ('Gini', -0.29539951573849876),\n",
       "                  ('AUC', 0.3523002421307506),\n",
       "                  ('Precision', 0.06521739130434782),\n",
       "                  ('F1', 0.11320754716981131),\n",
       "                  ('Balanced Accuracy Binary', 0.3498789346246973),\n",
       "                  ('Accuracy Binary', 0.2878787878787879),\n",
       "                  ('# Training', 134),\n",
       "                  ('# Validation', 66)]),\n",
       "     'mean_cv_score': 6.460743741934591,\n",
       "     'binary_classification_threshold': 0.1568627424310231}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 25.52541256225873,\n",
       "    'MCC Binary': inf,\n",
       "    'Gini': inf,\n",
       "    'AUC': 11.743341404358354,\n",
       "    'Precision': 55.5072463768116,\n",
       "    'F1': 44.79922593130141,\n",
       "    'Balanced Accuracy Binary': 11.097659402744153,\n",
       "    'Accuracy Binary': -17.21694557515452},\n",
       "   'percent_better_than_baseline': 25.52541256225873,\n",
       "   'validation_score': 0.651556943513563},\n",
       "  3: {'id': 3,\n",
       "   'pipeline_name': 'Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler',\n",
       "   'pipeline_class': evalml.pipelines.binary_classification_pipeline.BinaryClassificationPipeline,\n",
       "   'pipeline_summary': 'Random Forest Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour'],\n",
       "     'encode_as_categories': False,\n",
       "     'date_index': None},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': 'if_binary',\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'SMOTENC Oversampler': {'sampling_ratio': 0.25,\n",
       "     'k_neighbors_default': 5,\n",
       "     'n_jobs': -1,\n",
       "     'sampling_ratio_dict': None,\n",
       "     'categorical_features': [3,\n",
       "      10,\n",
       "      11,\n",
       "      12,\n",
       "      13,\n",
       "      14,\n",
       "      15,\n",
       "      16,\n",
       "      17,\n",
       "      18,\n",
       "      19,\n",
       "      20,\n",
       "      21,\n",
       "      22,\n",
       "      23,\n",
       "      24,\n",
       "      25,\n",
       "      26,\n",
       "      27,\n",
       "      28,\n",
       "      29,\n",
       "      30,\n",
       "      31,\n",
       "      32,\n",
       "      33,\n",
       "      34,\n",
       "      35,\n",
       "      36,\n",
       "      37,\n",
       "      38,\n",
       "      39,\n",
       "      40,\n",
       "      41,\n",
       "      42,\n",
       "      43,\n",
       "      44,\n",
       "      45,\n",
       "      46,\n",
       "      47,\n",
       "      48,\n",
       "      49,\n",
       "      50,\n",
       "      51,\n",
       "      52,\n",
       "      53,\n",
       "      54,\n",
       "      55,\n",
       "      56,\n",
       "      57,\n",
       "      58,\n",
       "      59],\n",
       "     'k_neighbors': 5},\n",
       "    'Random Forest Classifier': {'n_estimators': 100,\n",
       "     'max_depth': 6,\n",
       "     'n_jobs': -1}},\n",
       "   'mean_cv_score': 0.2856133806139974,\n",
       "   'standard_deviation_cv_score': 0.041115539258178,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 2.2374391555786133,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.26369520661129536),\n",
       "                  ('MCC Binary', 0.47636443708895493),\n",
       "                  ('Gini', 0.6483050847457628),\n",
       "                  ('AUC', 0.8241525423728814),\n",
       "                  ('Precision', 1.0),\n",
       "                  ('F1', 0.4),\n",
       "                  ('Balanced Accuracy Binary', 0.625),\n",
       "                  ('Accuracy Binary', 0.9104477611940298),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.26369520661129536,\n",
       "     'binary_classification_threshold': 0.4988241268809967},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.33304413887923806),\n",
       "                  ('MCC Binary', 0.4900218379501181),\n",
       "                  ('Gini', 0.423728813559322),\n",
       "                  ('AUC', 0.711864406779661),\n",
       "                  ('Precision', 0.75),\n",
       "                  ('F1', 0.5),\n",
       "                  ('Balanced Accuracy Binary', 0.6790254237288136),\n",
       "                  ('Accuracy Binary', 0.9104477611940298),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.33304413887923806,\n",
       "     'binary_classification_threshold': 0.5065550313835498},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.2601007963514588),\n",
       "                  ('MCC Binary', 0.6335302236023843),\n",
       "                  ('Gini', 0.757869249394673),\n",
       "                  ('AUC', 0.8789346246973365),\n",
       "                  ('Precision', 1.0),\n",
       "                  ('F1', 0.6),\n",
       "                  ('Balanced Accuracy Binary', 0.7142857142857143),\n",
       "                  ('Accuracy Binary', 0.9393939393939394),\n",
       "                  ('# Training', 134),\n",
       "                  ('# Validation', 66)]),\n",
       "     'mean_cv_score': 0.2601007963514588,\n",
       "     'binary_classification_threshold': 0.4459844896470452}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 92.80647509992905,\n",
       "    'MCC Binary': inf,\n",
       "    'Gini': inf,\n",
       "    'AUC': 30.498385794995965,\n",
       "    'Precision': 91.66666666666666,\n",
       "    'F1': 50.0,\n",
       "    'Balanced Accuracy Binary': 17.277037933817596,\n",
       "    'Accuracy Binary': 3.505201266395308},\n",
       "   'percent_better_than_baseline': 92.80647509992905,\n",
       "   'validation_score': 0.26369520661129536},\n",
       "  4: {'id': 4,\n",
       "   'pipeline_name': 'LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler',\n",
       "   'pipeline_class': evalml.pipelines.binary_classification_pipeline.BinaryClassificationPipeline,\n",
       "   'pipeline_summary': 'LightGBM Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour'],\n",
       "     'encode_as_categories': False,\n",
       "     'date_index': None},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': 'if_binary',\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'SMOTENC Oversampler': {'sampling_ratio': 0.25,\n",
       "     'k_neighbors_default': 5,\n",
       "     'n_jobs': -1,\n",
       "     'sampling_ratio_dict': None,\n",
       "     'categorical_features': [3,\n",
       "      10,\n",
       "      11,\n",
       "      12,\n",
       "      13,\n",
       "      14,\n",
       "      15,\n",
       "      16,\n",
       "      17,\n",
       "      18,\n",
       "      19,\n",
       "      20,\n",
       "      21,\n",
       "      22,\n",
       "      23,\n",
       "      24,\n",
       "      25,\n",
       "      26,\n",
       "      27,\n",
       "      28,\n",
       "      29,\n",
       "      30,\n",
       "      31,\n",
       "      32,\n",
       "      33,\n",
       "      34,\n",
       "      35,\n",
       "      36,\n",
       "      37,\n",
       "      38,\n",
       "      39,\n",
       "      40,\n",
       "      41,\n",
       "      42,\n",
       "      43,\n",
       "      44,\n",
       "      45,\n",
       "      46,\n",
       "      47,\n",
       "      48,\n",
       "      49,\n",
       "      50,\n",
       "      51,\n",
       "      52,\n",
       "      53,\n",
       "      54,\n",
       "      55,\n",
       "      56,\n",
       "      57,\n",
       "      58,\n",
       "      59],\n",
       "     'k_neighbors': 5},\n",
       "    'LightGBM Classifier': {'boosting_type': 'gbdt',\n",
       "     'learning_rate': 0.1,\n",
       "     'n_estimators': 100,\n",
       "     'max_depth': 0,\n",
       "     'num_leaves': 31,\n",
       "     'min_child_samples': 20,\n",
       "     'n_jobs': -1,\n",
       "     'bagging_freq': 0,\n",
       "     'bagging_fraction': 0.9}},\n",
       "   'mean_cv_score': 0.30863594948686357,\n",
       "   'standard_deviation_cv_score': 0.20387814963979614,\n",
       "   'high_variance_cv': True,\n",
       "   'training_time': 2.1367740631103516,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.23494740261438743),\n",
       "                  ('MCC Binary', 0.4900218379501181),\n",
       "                  ('Gini', 0.7881355932203391),\n",
       "                  ('AUC', 0.8940677966101696),\n",
       "                  ('Precision', 0.75),\n",
       "                  ('F1', 0.5),\n",
       "                  ('Balanced Accuracy Binary', 0.6790254237288136),\n",
       "                  ('Accuracy Binary', 0.9104477611940298),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.23494740261438743,\n",
       "     'binary_classification_threshold': 0.5037478165498673},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.5391133772263208),\n",
       "                  ('MCC Binary', 0.4900218379501181),\n",
       "                  ('Gini', 0.27118644067796605),\n",
       "                  ('AUC', 0.635593220338983),\n",
       "                  ('Precision', 0.75),\n",
       "                  ('F1', 0.5),\n",
       "                  ('Balanced Accuracy Binary', 0.6790254237288136),\n",
       "                  ('Accuracy Binary', 0.9104477611940298),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.5391133772263208,\n",
       "     'binary_classification_threshold': 0.7736716431132877},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.15184706861988254),\n",
       "                  ('MCC Binary', 0.3600976668493281),\n",
       "                  ('Gini', 0.7869249394673123),\n",
       "                  ('AUC', 0.8934624697336562),\n",
       "                  ('Precision', 1.0),\n",
       "                  ('F1', 0.25),\n",
       "                  ('Balanced Accuracy Binary', 0.5714285714285714),\n",
       "                  ('Accuracy Binary', 0.9090909090909091),\n",
       "                  ('# Training', 134),\n",
       "                  ('# Validation', 66)]),\n",
       "     'mean_cv_score': 0.15184706861988254,\n",
       "     'binary_classification_threshold': 0.9385567379765596}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 92.22662333635083,\n",
       "    'MCC Binary': inf,\n",
       "    'Gini': inf,\n",
       "    'AUC': 30.77078288942697,\n",
       "    'Precision': 83.33333333333334,\n",
       "    'F1': 41.66666666666667,\n",
       "    'Balanced Accuracy Binary': 14.315980629539949,\n",
       "    'Accuracy Binary': 2.4951002562942914},\n",
       "   'percent_better_than_baseline': 92.22662333635083,\n",
       "   'validation_score': 0.23494740261438743},\n",
       "  5: {'id': 5,\n",
       "   'pipeline_name': 'Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler + Standard Scaler',\n",
       "   'pipeline_class': evalml.pipelines.binary_classification_pipeline.BinaryClassificationPipeline,\n",
       "   'pipeline_summary': 'Logistic Regression Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler + Standard Scaler',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour'],\n",
       "     'encode_as_categories': False,\n",
       "     'date_index': None},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': 'if_binary',\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'SMOTENC Oversampler': {'sampling_ratio': 0.25,\n",
       "     'k_neighbors_default': 5,\n",
       "     'n_jobs': -1,\n",
       "     'sampling_ratio_dict': None,\n",
       "     'categorical_features': [3,\n",
       "      10,\n",
       "      11,\n",
       "      12,\n",
       "      13,\n",
       "      14,\n",
       "      15,\n",
       "      16,\n",
       "      17,\n",
       "      18,\n",
       "      19,\n",
       "      20,\n",
       "      21,\n",
       "      22,\n",
       "      23,\n",
       "      24,\n",
       "      25,\n",
       "      26,\n",
       "      27,\n",
       "      28,\n",
       "      29,\n",
       "      30,\n",
       "      31,\n",
       "      32,\n",
       "      33,\n",
       "      34,\n",
       "      35,\n",
       "      36,\n",
       "      37,\n",
       "      38,\n",
       "      39,\n",
       "      40,\n",
       "      41,\n",
       "      42,\n",
       "      43,\n",
       "      44,\n",
       "      45,\n",
       "      46,\n",
       "      47,\n",
       "      48,\n",
       "      49,\n",
       "      50,\n",
       "      51,\n",
       "      52,\n",
       "      53,\n",
       "      54,\n",
       "      55,\n",
       "      56,\n",
       "      57,\n",
       "      58,\n",
       "      59],\n",
       "     'k_neighbors': 5},\n",
       "    'Logistic Regression Classifier': {'penalty': 'l2',\n",
       "     'C': 1.0,\n",
       "     'n_jobs': -1,\n",
       "     'multi_class': 'auto',\n",
       "     'solver': 'lbfgs'}},\n",
       "   'mean_cv_score': 0.5519600330273683,\n",
       "   'standard_deviation_cv_score': 0.08269486722270435,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 4.9166669845581055,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.6286458411705715),\n",
       "                  ('MCC Binary', 0.17518582316850065),\n",
       "                  ('Gini', 0.1652542372881356),\n",
       "                  ('AUC', 0.5826271186440678),\n",
       "                  ('Precision', 0.2857142857142857),\n",
       "                  ('F1', 0.26666666666666666),\n",
       "                  ('Balanced Accuracy Binary', 0.5826271186440678),\n",
       "                  ('Accuracy Binary', 0.835820895522388),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.6286458411705715,\n",
       "     'binary_classification_threshold': 0.4313366338786227},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.4643433409235018),\n",
       "                  ('MCC Binary', 0.14283901342792224),\n",
       "                  ('Gini', 0.4067796610169492),\n",
       "                  ('AUC', 0.7033898305084746),\n",
       "                  ('Precision', 0.3333333333333333),\n",
       "                  ('F1', 0.18181818181818182),\n",
       "                  ('Balanced Accuracy Binary', 0.5455508474576272),\n",
       "                  ('Accuracy Binary', 0.8656716417910447),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.4643433409235018,\n",
       "     'binary_classification_threshold': 0.7298626702409028},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.5628909169880317),\n",
       "                  ('MCC Binary', 0.13468820490061437),\n",
       "                  ('Gini', 0.21549636803874073),\n",
       "                  ('AUC', 0.6077481840193704),\n",
       "                  ('Precision', 0.17647058823529413),\n",
       "                  ('F1', 0.25),\n",
       "                  ('Balanced Accuracy Binary', 0.5956416464891041),\n",
       "                  ('Accuracy Binary', 0.7272727272727273),\n",
       "                  ('# Training', 134),\n",
       "                  ('# Validation', 66)]),\n",
       "     'mean_cv_score': 0.5628909169880317,\n",
       "     'binary_classification_threshold': 0.16920770831511384}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 86.0982064884681,\n",
       "    'MCC Binary': inf,\n",
       "    'Gini': inf,\n",
       "    'AUC': 13.125504439063763,\n",
       "    'Precision': 26.517273576097107,\n",
       "    'F1': 23.282828282828284,\n",
       "    'Balanced Accuracy Binary': 7.4606537530266355,\n",
       "    'Accuracy Binary': -7.5456053067993185},\n",
       "   'percent_better_than_baseline': 86.0982064884681,\n",
       "   'validation_score': 0.6286458411705715},\n",
       "  6: {'id': 6,\n",
       "   'pipeline_name': 'XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler',\n",
       "   'pipeline_class': evalml.pipelines.binary_classification_pipeline.BinaryClassificationPipeline,\n",
       "   'pipeline_summary': 'XGBoost Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour'],\n",
       "     'encode_as_categories': False,\n",
       "     'date_index': None},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': 'if_binary',\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'SMOTENC Oversampler': {'sampling_ratio': 0.25,\n",
       "     'k_neighbors_default': 5,\n",
       "     'n_jobs': -1,\n",
       "     'sampling_ratio_dict': None},\n",
       "    'XGBoost Classifier': {'eta': 0.1,\n",
       "     'max_depth': 6,\n",
       "     'min_child_weight': 1,\n",
       "     'n_estimators': 100,\n",
       "     'n_jobs': -1}},\n",
       "   'mean_cv_score': nan,\n",
       "   'standard_deviation_cv_score': nan,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 1.5611882209777832,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary', nan),\n",
       "                  ('MCC Binary', nan),\n",
       "                  ('Gini', nan),\n",
       "                  ('AUC', nan),\n",
       "                  ('Precision', nan),\n",
       "                  ('F1', nan),\n",
       "                  ('Balanced Accuracy Binary', nan),\n",
       "                  ('Accuracy Binary', nan),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': nan,\n",
       "     'binary_classification_threshold': None},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary', nan),\n",
       "                  ('MCC Binary', nan),\n",
       "                  ('Gini', nan),\n",
       "                  ('AUC', nan),\n",
       "                  ('Precision', nan),\n",
       "                  ('F1', nan),\n",
       "                  ('Balanced Accuracy Binary', nan),\n",
       "                  ('Accuracy Binary', nan),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': nan,\n",
       "     'binary_classification_threshold': None},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary', nan),\n",
       "                  ('MCC Binary', nan),\n",
       "                  ('Gini', nan),\n",
       "                  ('AUC', nan),\n",
       "                  ('Precision', nan),\n",
       "                  ('F1', nan),\n",
       "                  ('Balanced Accuracy Binary', nan),\n",
       "                  ('Accuracy Binary', nan),\n",
       "                  ('# Training', 134),\n",
       "                  ('# Validation', 66)]),\n",
       "     'mean_cv_score': nan,\n",
       "     'binary_classification_threshold': None}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': nan,\n",
       "    'MCC Binary': nan,\n",
       "    'Gini': nan,\n",
       "    'AUC': nan,\n",
       "    'Precision': nan,\n",
       "    'F1': nan,\n",
       "    'Balanced Accuracy Binary': nan,\n",
       "    'Accuracy Binary': nan},\n",
       "   'percent_better_than_baseline': nan,\n",
       "   'validation_score': nan},\n",
       "  7: {'id': 7,\n",
       "   'pipeline_name': 'Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler',\n",
       "   'pipeline_class': evalml.pipelines.binary_classification_pipeline.BinaryClassificationPipeline,\n",
       "   'pipeline_summary': 'Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder + SMOTENC Oversampler',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour'],\n",
       "     'encode_as_categories': False,\n",
       "     'date_index': None},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': 'if_binary',\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'SMOTENC Oversampler': {'sampling_ratio': 0.25,\n",
       "     'k_neighbors_default': 5,\n",
       "     'n_jobs': -1,\n",
       "     'sampling_ratio_dict': None,\n",
       "     'categorical_features': [3,\n",
       "      10,\n",
       "      11,\n",
       "      12,\n",
       "      13,\n",
       "      14,\n",
       "      15,\n",
       "      16,\n",
       "      17,\n",
       "      18,\n",
       "      19,\n",
       "      20,\n",
       "      21,\n",
       "      22,\n",
       "      23,\n",
       "      24,\n",
       "      25,\n",
       "      26,\n",
       "      27,\n",
       "      28,\n",
       "      29,\n",
       "      30,\n",
       "      31,\n",
       "      32,\n",
       "      33,\n",
       "      34,\n",
       "      35,\n",
       "      36,\n",
       "      37,\n",
       "      38,\n",
       "      39,\n",
       "      40,\n",
       "      41,\n",
       "      42,\n",
       "      43,\n",
       "      44,\n",
       "      45,\n",
       "      46,\n",
       "      47,\n",
       "      48,\n",
       "      49,\n",
       "      50,\n",
       "      51,\n",
       "      52,\n",
       "      53,\n",
       "      54,\n",
       "      55,\n",
       "      56,\n",
       "      57,\n",
       "      58,\n",
       "      59],\n",
       "     'k_neighbors': 5},\n",
       "    'Extra Trees Classifier': {'n_estimators': 100,\n",
       "     'max_features': 'auto',\n",
       "     'max_depth': 6,\n",
       "     'min_samples_split': 2,\n",
       "     'min_weight_fraction_leaf': 0.0,\n",
       "     'n_jobs': -1}},\n",
       "   'mean_cv_score': 0.33828572013370833,\n",
       "   'standard_deviation_cv_score': 0.009380537483031018,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 2.055055856704712,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.32901507195041363),\n",
       "                  ('MCC Binary', 0.3624510999859961),\n",
       "                  ('Gini', 0.5677966101694916),\n",
       "                  ('AUC', 0.7838983050847458),\n",
       "                  ('Precision', 0.4),\n",
       "                  ('F1', 0.4444444444444445),\n",
       "                  ('Balanced Accuracy Binary', 0.6991525423728814),\n",
       "                  ('Accuracy Binary', 0.8507462686567164),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.32901507195041363,\n",
       "     'binary_classification_threshold': 0.24464061936922427},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.3380696737832299),\n",
       "                  ('MCC Binary', -0.045325960909933655),\n",
       "                  ('Gini', 0.4279661016949152),\n",
       "                  ('AUC', 0.7139830508474576),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.4915254237288136),\n",
       "                  ('Accuracy Binary', 0.8656716417910447),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.3380696737832299,\n",
       "     'binary_classification_threshold': 0.3074199530907704},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.3477724146674814),\n",
       "                  ('MCC Binary', 0.27835560025568845),\n",
       "                  ('Gini', 0.43825665859564156),\n",
       "                  ('AUC', 0.7191283292978208),\n",
       "                  ('Precision', 0.22727272727272727),\n",
       "                  ('F1', 0.3448275862068965),\n",
       "                  ('Balanced Accuracy Binary', 0.7130750605326877),\n",
       "                  ('Accuracy Binary', 0.7121212121212122),\n",
       "                  ('# Training', 134),\n",
       "                  ('# Validation', 66)]),\n",
       "     'mean_cv_score': 0.3477724146674814,\n",
       "     'binary_classification_threshold': 0.2199210188616361}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 91.47985733060223,\n",
       "    'MCC Binary': inf,\n",
       "    'Gini': inf,\n",
       "    'AUC': 23.900322841000797,\n",
       "    'Precision': 20.90909090909091,\n",
       "    'F1': 26.309067688378036,\n",
       "    'Balanced Accuracy Binary': 13.458434221146087,\n",
       "    'Accuracy Binary': -7.553143374038884},\n",
       "   'percent_better_than_baseline': 91.47985733060223,\n",
       "   'validation_score': 0.32901507195041363},\n",
       "  8: {'id': 8,\n",
       "   'pipeline_name': 'CatBoost Classifier w/ Imputer + DateTime Featurization Component + SMOTENC Oversampler',\n",
       "   'pipeline_class': evalml.pipelines.binary_classification_pipeline.BinaryClassificationPipeline,\n",
       "   'pipeline_summary': 'CatBoost Classifier w/ Imputer + DateTime Featurization Component + SMOTENC Oversampler',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour'],\n",
       "     'encode_as_categories': False,\n",
       "     'date_index': None},\n",
       "    'SMOTENC Oversampler': {'sampling_ratio': 0.25,\n",
       "     'k_neighbors_default': 5,\n",
       "     'n_jobs': -1,\n",
       "     'sampling_ratio_dict': None,\n",
       "     'categorical_features': [3, 4, 5, 6, 9, 10],\n",
       "     'k_neighbors': 5},\n",
       "    'CatBoost Classifier': {'n_estimators': 10,\n",
       "     'eta': 0.03,\n",
       "     'max_depth': 6,\n",
       "     'bootstrap_type': None,\n",
       "     'silent': True,\n",
       "     'allow_writing_files': False,\n",
       "     'n_jobs': -1}},\n",
       "   'mean_cv_score': 0.6018191346985708,\n",
       "   'standard_deviation_cv_score': 0.007246095254215931,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 0.9991879463195801,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.5936927052183711),\n",
       "                  ('MCC Binary', 0.7712055916006633),\n",
       "                  ('Gini', 0.7796610169491525),\n",
       "                  ('AUC', 0.8898305084745762),\n",
       "                  ('Precision', 1.0),\n",
       "                  ('F1', 0.7692307692307693),\n",
       "                  ('Balanced Accuracy Binary', 0.8125),\n",
       "                  ('Accuracy Binary', 0.9552238805970149),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.5936927052183711,\n",
       "     'binary_classification_threshold': 0.498623875482405},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.6076076767585368),\n",
       "                  ('MCC Binary', 0.47636443708895493),\n",
       "                  ('Gini', 0.12076271186440679),\n",
       "                  ('AUC', 0.5603813559322034),\n",
       "                  ('Precision', 1.0),\n",
       "                  ('F1', 0.4),\n",
       "                  ('Balanced Accuracy Binary', 0.625),\n",
       "                  ('Accuracy Binary', 0.9104477611940298),\n",
       "                  ('# Training', 133),\n",
       "                  ('# Validation', 67)]),\n",
       "     'mean_cv_score': 0.6076076767585368,\n",
       "     'binary_classification_threshold': 0.5007469614241519},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.6041570221188047),\n",
       "                  ('MCC Binary', 0.7469064529073725),\n",
       "                  ('Gini', 0.8692493946731235),\n",
       "                  ('AUC', 0.9346246973365617),\n",
       "                  ('Precision', 0.8333333333333334),\n",
       "                  ('F1', 0.7692307692307692),\n",
       "                  ('Balanced Accuracy Binary', 0.8486682808716708),\n",
       "                  ('Accuracy Binary', 0.9545454545454546),\n",
       "                  ('# Training', 134),\n",
       "                  ('# Validation', 66)]),\n",
       "     'mean_cv_score': 0.6041570221188047,\n",
       "     'binary_classification_threshold': 0.47437131050216075}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 84.84244358059615,\n",
       "    'MCC Binary': inf,\n",
       "    'Gini': inf,\n",
       "    'AUC': 29.49455205811139,\n",
       "    'Precision': 94.44444444444446,\n",
       "    'F1': 64.61538461538461,\n",
       "    'Balanced Accuracy Binary': 26.205609362389026,\n",
       "    'Accuracy Binary': 5.502789084878645},\n",
       "   'percent_better_than_baseline': 84.84244358059615,\n",
       "   'validation_score': 0.5936927052183711}},\n",
       " 'search_order': [0, 1, 2, 3, 4, 5, 6, 7, 8]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel AutoML\n",
    "\n",
    "By default, all pipelines in an AutoML batch are evaluated in series.  Pipelines can be evaluated in parallel to improve performance during AutoML search.  This is accomplished by a futures style submission and evaluation of pipelines in a batch.  As of this writing, the pipelines use a threaded model for concurrent evaluation.  This is similar to the currently implemented `n_jobs` parameter in the estimators, which uses increased numbers of threads to train and evaluate estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism with Concurrent Futures\n",
    "\n",
    "The `EngineBase` class is robust and extensible enough to support futures-like implementations from a variety of libraries.  The `CFEngine` extends the `EngineBase` to use the native Python concurrent.futures library.  The `CFEngine` supports both thread- and process-level parallelism.  The type of parallelism can be chosen using either the `ThreadPoolExecutor` or the `ProcessPoolExecutor`.  If either executor is passed a `max_workers` parameter, it will set the number of processes and threads spawned.  If not, the default number of processes will be equal to the number of processors available and the number of threads set to five times the number of processors available.\n",
    "\n",
    "Note: the cell demonstrating process-level parallelism is commented out due to incompatibility with our ReadTheDocs build.  It can be run successfully locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:56:20.143754Z",
     "start_time": "2021-07-27T18:56:15.172329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "2 pipelines ready for search.\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using CFEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 3 pipelines. \n",
      "Allowed model families: linear_model\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 12.868\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Elastic Net Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.077\n",
      "Logistic Regression Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.077\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLogistic Regression Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "\n",
      "Search finished after 00:03            \n",
      "Best pipeline: Logistic Regression Classifier w/ Imputer + Standard Scaler\n",
      "Best pipeline Log Loss Binary: 0.076807\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from evalml.automl.engine import CFEngine\n",
    "from evalml.automl.engine.cf_engine import CFClient\n",
    "\n",
    "# Use thread-level paralellism\n",
    "threaded_cf_engine = CFEngine(CFClient(ThreadPoolExecutor()))\n",
    "\n",
    "automl_cf_threaded = AutoMLSearch(X_train=X, y_train=y,\n",
    "                      problem_type=\"binary\",\n",
    "                      allowed_model_families=[ModelFamily.LINEAR_MODEL],\n",
    "                      engine = threaded_cf_engine)\n",
    "automl_cf_threaded.search(show_iteration_plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:56:20.147287Z",
     "start_time": "2021-07-27T18:56:20.145197Z"
    }
   },
   "outputs": [],
   "source": [
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# # Repeat the process but using process-level parallelism\n",
    "# process_cf_engine = CFEngine(CFClient(ProcessPoolExecutor()))\n",
    "\n",
    "# automl_cf_process = AutoMLSearch(X_train=X, y_train=y,\n",
    "#                       problem_type=\"binary\",\n",
    "#                       allowed_model_families=[ModelFamily.LINEAR_MODEL],\n",
    "#                       engine = process_cf_engine)\n",
    "# automl_cf_process.search(show_iteration_plot = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism with Dask\n",
    "\n",
    "Thread or process level parallelism can be explicitly evoked for the DaskEngine (as well as the CFEngine).  The `processes` can be set to `True` and the number of processes set using `n_workers`.  If `processes` is set to `False`, then the resulting parallelism will be threaded and `n_workers` will represent the threads used.  Examples of both will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:56:30.630746Z",
     "start_time": "2021-07-27T18:56:20.153029Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/distributed/node.py:160: UserWarning:\n",
      "\n",
      "Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 49984 instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "2 pipelines ready for search.\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 3 pipelines. \n",
      "Allowed model families: linear_model\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 12.868\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Elastic Net Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n",
      "Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.077\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLogistic Regression Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "\n",
      "Search finished after 00:09            \n",
      "Best pipeline: Logistic Regression Classifier w/ Imputer + Standard Scaler\n",
      "Best pipeline Log Loss Binary: 0.076807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "from evalml.automl.engine import DaskEngine\n",
    "\n",
    "dask_engine_t2 = DaskEngine(Client(LocalCluster(processes=True, n_workers = 2)))\n",
    "automl_dask_t2 = AutoMLSearch(X_train=X, y_train=y,\n",
    "                      problem_type=\"binary\",\n",
    "                      allowed_model_families=[ModelFamily.LINEAR_MODEL],\n",
    "                      engine = dask_engine_t2)\n",
    "automl_dask_t2.search(show_iteration_plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:56:35.392897Z",
     "start_time": "2021-07-27T18:56:30.633575Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/evalml_dev/lib/python3.9/site-packages/distributed/node.py:160: UserWarning:\n",
      "\n",
      "Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 50005 instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default limit of max_batches=1.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "2 pipelines ready for search.\n",
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using DaskEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 3 pipelines. \n",
      "Allowed model families: linear_model\n",
      "\n",
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 12.868\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Elastic Net Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.077\n",
      "Logistic Regression Classifier w/ Imputer + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.077\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLogistic Regression Classifier w/ Imputer + Standard Scaler may not perform as estimated on unseen data.\n",
      "\n",
      "Search finished after 00:04            \n",
      "Best pipeline: Logistic Regression Classifier w/ Imputer + Standard Scaler\n",
      "Best pipeline Log Loss Binary: 0.076807\n"
     ]
    }
   ],
   "source": [
    "dask_engine_p4 = DaskEngine(Client(LocalCluster(processes=False, n_workers = 4)))\n",
    "\n",
    "automl_dask_p4 = AutoMLSearch(X_train=X, y_train=y,\n",
    "                      problem_type=\"binary\",\n",
    "                      allowed_model_families=[ModelFamily.LINEAR_MODEL],\n",
    "                      engine = dask_engine_p4)\n",
    "automl_dask_p4.search(show_iteration_plot = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a significant performance gain can result from simply using something other than the default `SequentialEngine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T18:56:35.397855Z",
     "start_time": "2021-07-27T18:56:35.394488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential search duration: 19.669402837753296\n",
      "Concurrent futures (threaded) search duration: 3.979705810546875\n",
      "Dask (two threads) search duration: 9.009714126586914\n",
      "Dask (four processes)search duration: 4.219135046005249\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequential search duration: %s\" % str(automl.search_duration))\n",
    "print(\"Concurrent futures (threaded) search duration: %s\" % str(automl_cf_threaded.search_duration))\n",
    "# print(\"Concurrent futures (process) search duration: %s\" % str(automl_cf_process.search_duration))\n",
    "print(\"Dask (two threads) search duration: %s\" % str(automl_dask_t2.search_duration))\n",
    "print(\"Dask (four processes)search duration: %s\" % str(automl_dask_p4.search_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}